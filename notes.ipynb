{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some functions and libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in moodle mentioned:\n",
    "    for classifiers:\n",
    "        naive bayes # https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "        decision tree\n",
    "        knn\n",
    "        suport vector machines\n",
    "        logistic regression\n",
    "        ensemble methods\n",
    "    for evaluation: (all used in examples here)\n",
    "        accuracy\n",
    "        cross validation\n",
    "        confusion matrix\n",
    "        grid search\n",
    "    for text analysis:\n",
    "        feature extraction # https://scikit-learn.org/stable/modules/classes.html#text-feature-extraction-ref\n",
    "        NLTK # https://www.nltk.org/\n",
    "    some visualization\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confusion matrix, dummy encoding, knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isna().any() #Ist der Datensatz vollständig (d. h. gibt es keine Nullwerte)? missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:-1] #alle unabhängige var\n",
    "y=df.iloc[:,-1] #nur die outcome werte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, dummy_clf.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, dummy_clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn =KNeighborsClassifier()\n",
    "knn.fit(X_train,y_train)\n",
    "result =knn.predict(X_test)\n",
    "accuracy_score(y_test, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4,5,10,15,20,25,30,35]:\n",
    "  print(i)\n",
    "  knn =KNeighborsClassifier(n_neighbors=i)\n",
    "  knn.fit(X_train,y_train)\n",
    "  print(\"acc test \" +str(accuracy_score(y_test, knn.predict(X_test))))\n",
    "  print(\"acc train \" + str(accuracy_score(y_train, knn.predict(X_train))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn =KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Location'].unique())\n",
    "df[\"TRange\"]= df[\"MaxTemp\"] - df[\"MinTemp\"]\n",
    "df[\"MeanWind\"] = (df[\"WindSpeed3pm\"] + df[\"WindSpeed9am\"])/2\n",
    "#df[\"WindNormalized\"] = df[\"WindGustSpeed\"]/df[\"MeanWind\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bool_yes = {'Yes':1, \"No\":0}\n",
    "df[\"RainToday\"]= df[\"RainToday\"].map(bool_yes)\n",
    "df[\"RainTomorrow\"]= df[\"RainTomorrow\"].map(bool_yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train,y_train)\n",
    "accuracy_score(y_test, model.predict(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummyClass = DummyClassifier(strategy=\"stratified\")\n",
    "dummyClass.fit(X_train,y_train)\n",
    "accuracy_score(y_test, dummyClass.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_min_max_scaled= min_max_scaler.transform(X)\n",
    "normalizer = preprocessing.Normalizer()\n",
    "normalizer.fit(X_min_max_scaled)\n",
    "X_normalized= normalizer.transform(X_min_max_scaled)\n",
    "X_train_normalized, X_test_normalized, y_train_normalized, y_test_normalized = train_test_split(X_normalized, y, test_size=0.3)\n",
    "model_scaled = RandomForestClassifier(n_estimators=100)\n",
    "model_scaled.fit(X_train_normalized,y_train_normalized)\n",
    "prediction = model_scaled.predict(X_test_normalized)\n",
    "accuracy_score(y_test_normalized,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X.drop([\"Cloud3pm\",\"Location\"], axis=1)\n",
    "X = X[X[\"Cloud3pm\"]!=-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_transformer = Pipeline(steps=[('scale_01', preprocessing.MinMaxScaler()), \n",
    "                                            ('normalizer', preprocessing.Normalizer())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = RandomForestClassifier(n_estimators=100)\n",
    "my_pipeline = Pipeline(steps=[('preprocessing_transformer', preprocessing_transformer),\n",
    "                              ('model_pipeline', model_pipeline)\n",
    "                             ], verbose = True)\n",
    "#test train, fit, predict, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df[\"MaxTemp\"].values - df[\"MinTemp\"].values).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "def sub_12(X):\n",
    "    return np.reshape((X[:,0]-X[:,1]), (-1, 1))\n",
    "\n",
    "def mean_12(X):\n",
    "    return np.reshape(((X[:,0]+X[:,1])/2), (-1, 1))\n",
    "\n",
    "\n",
    "def identity_input(X):\n",
    "    return X\n",
    "\n",
    "preprocessor_extension = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Trange', FunctionTransformer(sub_12, validate=True), ['MaxTemp','MinTemp']),\n",
    "        ('DiffHum', FunctionTransformer(sub_12, validate=True), ['Humidity3pm','Humidity9am']  ),\n",
    "        ('MeanWind', FunctionTransformer(mean_12, validate=True), ['WindSpeed3pm','WindSpeed9am']  ),\n",
    "        ('all', FunctionTransformer(identity_input, validate=True), [cname for cname in X.columns]  )\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify=preprocessor_extension.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_extension = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('Trange', FunctionTransformer(sub_12, validate=True), ['MaxTemp','MinTemp']),\n",
    "        ('DiffHum', FunctionTransformer(sub_12, validate=True), ['Humidity3pm','Humidity9am']  ),\n",
    "        ('MeanWind', FunctionTransformer(mean_12, validate=True), ['WindSpeed3pm','WindSpeed9am']  ),\n",
    "        #('all', FunctionTransformer(identity_input, validate=True), [cname for cname in X.columns]  )\n",
    "    ], remainder ='passthrough')\n",
    "\n",
    "verify=preprocessor_extension.fit_transform(X)\n",
    "verify.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'model_pipeline__n_estimators': [10,100,250]\n",
    "    }\n",
    "gs_clf = GridSearchCV(my_pipeline, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "gs_clf.fit(X_train, y_train)\n",
    "\n",
    "gs_clf.best_params_\n",
    "my_pipeline.set_params(**gs_clf.best_params_)\n",
    "\n",
    "my_pipeline.fit(X_train,y_train)\n",
    "preds = my_pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "score = accuracy_score(y_test, preds)\n",
    "print('Accuracy Score:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "for i in mushroom.columns[1:]:\n",
    "    le.fit(mushroom.iloc[:,i])\n",
    "    mushroom.iloc[:,i]= le.transform(mushroom.iloc[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinal encoder\n",
    "oe = preprocessing.OrdinalEncoder()\n",
    "oe.fit_transform(mushroom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = []\n",
    "models.append(('LR', LogisticRegression()))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\n",
    "    cv_results = cross_val_score(model, X, y, cv=5, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\n",
    "    cv_results = cross_val_score(model, mushroom_color , y, cv=5, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom_color= pd.get_dummies(mushroom_color,  prefix_sep='_', columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "for name, model in models:\n",
    "\n",
    "    cv_results = cross_val_score(model, mushroom_color , y, cv=5, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "# boxplot algorithm comparison\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Algorithm Comparison')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encoding\n",
    "ohe = preprocessing.OneHotEncoder( sparse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "# fit predict accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## naive bayes\n",
    "# https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    ">>> from sklearn.datasets import load_iris\n",
    ">>> from sklearn.model_selection import train_test_split\n",
    ">>> from sklearn.naive_bayes import GaussianNB\n",
    ">>> X, y = load_iris(return_X_y=True)\n",
    ">>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    ">>> gnb = GaussianNB()\n",
    ">>> y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    ">>> print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "...       % (X_test.shape[0], (y_test != y_pred).sum()))\n",
    "Number of mislabeled points out of a total 75 points : 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## support vector machines svm\n",
    "# https://scikit-learn.org/stable/modules/svm.html\n",
    ">>> from sklearn import svm\n",
    ">>> X = [[0, 0], [1, 1]]\n",
    ">>> y = [0, 1]\n",
    ">>> clf = svm.SVC()\n",
    ">>> clf.fit(X, y)\n",
    "SVC()\n",
    ">>> clf.predict([[2., 2.]])\n",
    ">>> clf.support_vectors_\n",
    ">>> # get indices of support vectors\n",
    ">>> clf.support_\n",
    ">>> # get number of support vectors for each class\n",
    ">>> clf.n_support_\n",
    "\n",
    "## multi class classification\n",
    ">>> X = [[0], [1], [2], [3]]\n",
    ">>> Y = [0, 1, 2, 3]\n",
    ">>> clf = svm.SVC(decision_function_shape='ovo')\n",
    ">>> clf.fit(X, Y)\n",
    "SVC(decision_function_shape='ovo')\n",
    ">>> dec = clf.decision_function([[1]])\n",
    ">>> dec.shape[1] # 4 classes: 4*3/2 = 6\n",
    "6\n",
    ">>> clf.decision_function_shape = \"ovr\"\n",
    ">>> dec = clf.decision_function([[1]])\n",
    ">>> dec.shape[1] # 4 classes\n",
    "\n",
    "# docu for scaling and different kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.close(\"all\")\n",
    ".plot\n",
    ".plot.bar() - .diff().bar()\n",
    ".plot.hist() - .diff().hist()\n",
    ".plot.box()\n",
    ".boxplot()\n",
    ".plot.scatter()\n",
    ".plot.kde()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
